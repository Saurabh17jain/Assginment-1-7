{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfP5AiBzSoM_"
   },
   "source": [
    "Assignment 4\n",
    "Name: Saurabh kr jain\n",
    "Roll No: 102383677\n",
    "Subgroup: 3Co33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUZh3fHyR3ou"
   },
   "source": [
    "Q1- (Based on Step-by-Step Implementation of Ridge Regression using Gradient\n",
    "Descent Optimization)\n",
    "Generate a dataset with atleast seven highly correlated columns and a target variable.\n",
    "Implement Ridge Regression using Gradient Descent Optimization. Take different\n",
    "values of learning rate (such as 0.0001,0.001,0.01,0.1,1,10) and regularization\n",
    "parameter (10-15,10-10,10-5\n",
    ",10- 3\n",
    ",0,1,10,20). Choose the best parameters for which ridge\n",
    "regression cost function is minimum and R2_score is maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTqj_F8pEHne",
    "outputId": "0e847b46-e9ce-4fbc-df4b-567d3076c958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression (alpha=1e-15):\n",
      "Train R²: 0.9990, Test R²: 0.9987\n",
      "Train MSE: 0.0089, Test MSE: 0.0125\n",
      "\n",
      "Ridge Regression (alpha=1e-10):\n",
      "Train R²: 0.9990, Test R²: 0.9987\n",
      "Train MSE: 0.0089, Test MSE: 0.0125\n",
      "\n",
      "Ridge Regression (alpha=1e-05):\n",
      "Train R²: 0.9990, Test R²: 0.9987\n",
      "Train MSE: 0.0089, Test MSE: 0.0125\n",
      "\n",
      "Ridge Regression (alpha=0.001):\n",
      "Train R²: 0.9990, Test R²: 0.9987\n",
      "Train MSE: 0.0089, Test MSE: 0.0125\n",
      "\n",
      "Ridge Regression (alpha=0):\n",
      "Train R²: 0.9990, Test R²: 0.9987\n",
      "Train MSE: 0.0089, Test MSE: 0.0125\n",
      "\n",
      "Ridge Regression (alpha=1):\n",
      "Train R²: 0.9989, Test R²: 0.9987\n",
      "Train MSE: 0.0096, Test MSE: 0.0130\n",
      "\n",
      "Ridge Regression (alpha=10):\n",
      "Train R²: 0.9982, Test R²: 0.9987\n",
      "Train MSE: 0.0154, Test MSE: 0.0130\n",
      "\n",
      "Ridge Regression (alpha=20):\n",
      "Train R²: 0.9964, Test R²: 0.9976\n",
      "Train MSE: 0.0309, Test MSE: 0.0232\n",
      "\n",
      "Best alpha: 0\n",
      "Best Test R²: 0.9987\n",
      "Best Test MSE: 0.0125\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# (1) Data Preparation\n",
    "np.random.seed(42)\n",
    "num_samples = 100\n",
    "\n",
    "# Generate highly correlated features\n",
    "x1 = np.random.rand(num_samples)\n",
    "x2 = x1 + np.random.normal(0, 0.0001, num_samples)\n",
    "x3 = x1 + np.random.normal(0, 0.010, num_samples)\n",
    "x4 = x1 + np.random.normal(0, 0.01, num_samples)\n",
    "x5 = x1 + np.random.normal(0, 1, num_samples)\n",
    "x6 = x1 + np.random.normal(0, 1, num_samples)\n",
    "x7 = x1 + np.random.normal(0, 0, num_samples)\n",
    "\n",
    "# Create a target variable with some noise\n",
    "y = 5 * x1 + 3 * x2 + 2 * x3 + np.random.normal(0, 0.1, num_samples)\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'x1': x1,\n",
    "    'x2': x2,\n",
    "    'x3': x3,\n",
    "    'x4': x4,\n",
    "    'x5': x5,\n",
    "    'x6': x6,\n",
    "    'x7': x7,\n",
    "    'target': y\n",
    "})\n",
    "\n",
    "# (2) Split data into input features (X) and target (y)\n",
    "X = data.drop(columns=['target'])\n",
    "y = data['target']\n",
    "\n",
    "# (3) Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# (4) Standardize the features (important for Ridge Regression)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# (5) Define the list of regularization parameters\n",
    "alphas = [1e-15, 1e-10, 1e-5, 1e-3, 0, 1, 10, 20]\n",
    "\n",
    "# (6) Apply Ridge Regression for each regularization parameter\n",
    "results = []\n",
    "for alpha in alphas:\n",
    "    # Initialize and fit the Ridge Regression model\n",
    "    ridge_reg = Ridge(alpha=alpha)\n",
    "    ridge_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_train = ridge_reg.predict(X_train)\n",
    "    y_pred_test = ridge_reg.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'alpha': alpha,\n",
    "        'train_r2': r2_train,\n",
    "        'test_r2': r2_test,\n",
    "        'train_mse': mse_train,\n",
    "        'test_mse': mse_test\n",
    "    })\n",
    "\n",
    "    # Output the results for the current alpha\n",
    "    print(f\"Ridge Regression (alpha={alpha}):\")\n",
    "    print(f\"Train R²: {r2_train:.4f}, Test R²: {r2_test:.4f}\")\n",
    "    print(f\"Train MSE: {mse_train:.4f}, Test MSE: {mse_test:.4f}\\n\")\n",
    "\n",
    "# (7) Find the best alpha based on test R² score\n",
    "best_result = max(results, key=lambda x: x['test_r2'])\n",
    "\n",
    "print(f\"Best alpha: {best_result['alpha']}\")\n",
    "print(f\"Best Test R²: {best_result['test_r2']:.4f}\")\n",
    "print(f\"Best Test MSE: {best_result['test_mse']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr-hMhgBSAeX"
   },
   "source": [
    "Q2- Load the Hitters dataset from the following link\n",
    "https://drive.google.com/file/d/1qzCKF6JKKMB0p7ul_lLy8tdmRk3vE_bG/view?usp=sharing\n",
    "(a) Pre-process the data (null values, noise, categorical to numerical encoding)\n",
    "(b) Separate input and output features and perform scaling\n",
    "(c) Fit a Linear, Ridge (use regularization parameter as 0.5748), and LASSO (use\n",
    "regularization parameter as 0.5748) regression function on the dataset.\n",
    "(d) Evaluate the performance of each trained model on test set. Which model performs\n",
    "the best and Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWfehO_NKp0W",
    "outputId": "16d35e97-353f-4bf1-b75d-d1b969417114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - MSE: 128284.34549672354 R2 Score: 0.290745185579814\n",
      "Ridge Regression - MSE: 126603.90264424692 R2 Score: 0.30003596988293446\n",
      "Lasso Regression - MSE: 126739.56899132291 R2 Score: 0.29928590166965496\n",
      "The best performing model is: Ridge Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.185e+04, tolerance: 4.367e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "url = 'https://raw.githubusercontent.com/selva86/datasets/master/Hitters.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# (a) Pre-process the data\n",
    "# Remove rows with null values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert categorical data to numerical encoding using one-hot encoding\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# (b) Separate input and output features and perform scaling\n",
    "X = df.drop('Salary', axis=1)  # Input features\n",
    "y = df['Salary']  # Output feature\n",
    "\n",
    "# Train-test split (80% training and 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform scaling on input features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# (c) Fit a Linear, Ridge, and Lasso regression function on the dataset\n",
    "# Linear Regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Ridge Regression with regularization parameter = 0.5748\n",
    "ridge_reg = Ridge(alpha=0.5748)\n",
    "ridge_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Lasso Regression with regularization parameter = 0.5748\n",
    "lasso_reg = Lasso(alpha=0.5748)\n",
    "lasso_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# (d) Evaluate performance of each model on test set\n",
    "# Function to calculate performance metrics\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return mse, r2\n",
    "\n",
    "# Evaluate Linear Regression\n",
    "mse_lin, r2_lin = evaluate_model(lin_reg, X_test_scaled, y_test)\n",
    "\n",
    "# Evaluate Ridge Regression\n",
    "mse_ridge, r2_ridge = evaluate_model(ridge_reg, X_test_scaled, y_test)\n",
    "\n",
    "# Evaluate Lasso Regression\n",
    "mse_lasso, r2_lasso = evaluate_model(lasso_reg, X_test_scaled, y_test)\n",
    "\n",
    "# Print performance of each model\n",
    "print(\"Linear Regression - MSE:\", mse_lin, \"R2 Score:\", r2_lin)\n",
    "print(\"Ridge Regression - MSE:\", mse_ridge, \"R2 Score:\", r2_ridge)\n",
    "print(\"Lasso Regression - MSE:\", mse_lasso, \"R2 Score:\", r2_lasso)\n",
    "\n",
    "# Find which model performs the best\n",
    "if mse_lin < mse_ridge and mse_lin < mse_lasso:\n",
    "    best_model = \"Linear Regression\"\n",
    "elif mse_ridge < mse_lin and mse_ridge < mse_lasso:\n",
    "    best_model = \"Ridge Regression\"\n",
    "else:\n",
    "    best_model = \"Lasso Regression\"\n",
    "\n",
    "print(f\"The best performing model is: {best_model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGeZn3f5SN1y"
   },
   "source": [
    "Q3- Cross Validation for Ridge and Lasso Regression\n",
    "Explore Ridge Cross Validation (RidgeCV) and Lasso Cross Validation (LassoCV)\n",
    "function of Python. Implement both on Boston House Prediction Dataset (load_boston\n",
    "dataset from sklearn.datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QDaIArHlL-vL",
    "outputId": "b66a433d-3c52-4a11-c386-8ff30bc5037b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_ridge.py:2341: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha for RidgeCV: 2.009233002565046\n",
      "RidgeCV - MSE: 0.5558180238596643 R2 Score: 0.5758438525003107\n",
      "Best alpha for LassoCV: 0.0006135907273413176\n",
      "LassoCV - MSE: 0.5550152557455887 R2 Score: 0.5764564613326717\n",
      "LassoCV performs better with lower MSE.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import fetch_california_housing  # Alternative to deprecated load_boston\n",
    "\n",
    "# Load the dataset\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name=\"Price\")\n",
    "\n",
    "# Train-test split (80% training and 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform scaling on input features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# (a) Implement RidgeCV for Ridge Regression with cross-validation\n",
    "# RidgeCV automatically selects the best alpha from a list of alphas using cross-validation\n",
    "alphas = np.logspace(-6, 6, 100)  # Logarithmic range of alphas to search\n",
    "ridge_cv = RidgeCV(alphas=alphas, store_cv_values=True)  # store_cv_values for diagnostic purposes\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate RidgeCV model\n",
    "y_pred_ridge = ridge_cv.predict(X_test_scaled)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(\"Best alpha for RidgeCV:\", ridge_cv.alpha_)\n",
    "print(\"RidgeCV - MSE:\", mse_ridge, \"R2 Score:\", r2_ridge)\n",
    "\n",
    "# (b) Implement LassoCV for Lasso Regression with cross-validation\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=42)  # cv=5 for 5-fold cross-validation\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate LassoCV model\n",
    "y_pred_lasso = lasso_cv.predict(X_test_scaled)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "print(\"Best alpha for LassoCV:\", lasso_cv.alpha_)\n",
    "print(\"LassoCV - MSE:\", mse_lasso, \"R2 Score:\", r2_lasso)\n",
    "\n",
    "# Print the best performing model based on MSE\n",
    "if mse_ridge < mse_lasso:\n",
    "    print(\"RidgeCV performs better with lower MSE.\")\n",
    "else:\n",
    "    print(\"LassoCV performs better with lower MSE.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
